#source、channel和sink的简称
agent.sources = s1
agent.sinks=k1
#指定source类型
agent.sources.s1.type=com.zhw.flume.source.tail.TailDirSource
#用于记录source消费记录的文件
agent.sources.s1.positionFile = /home/shared/log/taildir_position.json
#source线程最大休眠时间，默认是5s，测试后发现默认值不太合适，建议调小一些，这里设置为1s
agent.sources.s1.maxBackoffSleep = 1000
#source每次读取的日志数量（一行记作一条)，默认是一百，测试发现适当调高这个值可以取得更好的性能
agent.sources.s1.batchSize=100
#为了避免在监听多个日志时，source一直读取一个日志的信息，需要设置每个日志最多读取的条数
agent.sources.s1.maxBatchCount=10000
#监听的文件组
agent.sources.s1.filegroups=f1
#可以使用正则的方式进行日志文件匹配
agent.sources.s1.filegroups.f1 = /home/shared/log/^tutor-live-data-abnormal.log.*
agent.sources.s1.headers.f1.HOST=host
# 指定新文件的日志是否直接跳到尾部，这样设置可能有点问题，每天晚上日志归档后，
# 新日志文件被首次监听到时，被监听到前的日志写入数据会丢失，先这样写，
# 因为是使用的正则匹配的方式来监听的日志文件，不这样写，以前的文件都会读取了
# 后边需要再对Source和Reader进行修改
agent.sources.s1.skipToEnd=true

#设置Kafka接收器
agent.sinks.k1.type = com.zhw.flume.sink.kafka.KafkaSink
#设置kafka每次读取的数量
agent.sinks.k1.flumeBatchSize=100
#设置Kafka的broker地址和端口号
agent.sinks.k1.brokerList=localhost:9092
#设置Kafka的Topic，kafka的topic直接配置文件指定且不允许重写
agent.sinks.k1.allowTopicOverride=false
agent.sinks.k1.kafka.topic=test
#设置序列化方式
agent.sinks.k1.serializer.class=kafka.serializer.StringEncoder